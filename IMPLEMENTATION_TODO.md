# SurfCastAI Implementation TODO - Based on Recent Live Testing

## Current Status Assessment (October 4, 2025)

Based on the successful live test completed on October 3, 2025, the system is **PRODUCTION READY** with the following status:

**‚úÖ WORKING PERFECTLY:**
- GPT-5-mini model for image analysis (already using the recommended reliable model)
- GPT-5-nano model for text generation
- All critical bugs fixed (visualization.py, timeout handling, debug files)
- 87% data collection success rate (external API failures, not our bugs)
- Professional-quality forecast generation
- $0.0066/forecast cost (well under budget)

**‚ö†Ô∏è NEEDS ATTENTION:**
- API key security (exposed in config.yaml)
- Missing cost tracking from response.usage
- Weather agent not creating JSON files (external API 404s)
- Wave model agent not creating JSON files (external API timeouts)

**üí° NICE TO HAVE:**
- Structured outputs using Pydantic schemas
- Circuit breaker for API calls
- Enhanced monitoring and alerting

---

## Priority 1: CRITICAL SECURITY & IMMEDIATE FIXES üî¥

### 1.1 Fix API Key Security (URGENT - Before Production Deployment)
**Status:** ‚ùå CRITICAL - API key exposed in version control
**Priority:** P0 - MUST FIX BEFORE PRODUCTION
**Effort:** 15 minutes
**Risk:** HIGH - Current API key is compromised

**Tasks:**
- [ ] Remove hardcoded API key from config/config.yaml line 8
- [ ] Update config to use environment variable:
  ```yaml
  openai:
    api_key: ${OPENAI_API_KEY}
  ```
- [ ] Create .env.example file with placeholder:
  ```
  OPENAI_API_KEY=your-api-key-here
  ```
- [ ] Add .env to .gitignore (if not already)
- [ ] Update README with environment setup instructions
- [ ] **ROTATE THE EXPOSED API KEY IMMEDIATELY** via OpenAI dashboard

**Verification:**
```bash
# Check config.yaml doesn't contain sk-
grep -n "sk-" config/config.yaml  # Should return nothing
```

---

### 1.2 Add Token Usage & Cost Tracking
**Status:** ‚ö†Ô∏è Missing - Currently estimating costs
**Priority:** P1 - Important for production monitoring
**Effort:** 30 minutes
**Risk:** LOW

**Tasks:**
- [ ] Update `_call_openai_api()` to track token usage from `response.usage`
- [ ] Calculate actual costs using official pricing:
  - GPT-5-mini: $0.25/1M input, $2/1M output
  - GPT-5-nano: $0.05/1M input, $0.40/1M output
- [ ] Log costs per API call
- [ ] Aggregate total cost per forecast
- [ ] Save cost data to forecast metadata JSON

**Implementation:**
```python
# In _call_openai_api() after getting response
if hasattr(response, 'usage') and response.usage:
    input_tokens = response.usage.prompt_tokens
    output_tokens = response.usage.completion_tokens

    # Calculate cost based on model
    if 'gpt-5-mini' in self.openai_model:
        cost = (input_tokens * 0.00025 + output_tokens * 0.002) / 1000
    elif 'gpt-5-nano' in self.openai_model:
        cost = (input_tokens * 0.00005 + output_tokens * 0.0004) / 1000
    else:
        cost = 0

    self.logger.info(f"API call: {input_tokens} input + {output_tokens} output = ${cost:.4f}")

    # Accumulate in instance variable for forecast total
    if not hasattr(self, 'total_cost'):
        self.total_cost = 0
    self.total_cost += cost
```

**Verification:**
```bash
# Check logs for cost tracking
grep "API call:" /tmp/final_live_test.log
```

---

## Priority 2: EXTERNAL API ISSUES (NOT OUR BUGS) üü°

### 2.1 Weather API 404 Errors
**Status:** ‚ö†Ô∏è External API issue - NWS gridpoints return 404
**Priority:** P2 - System works without weather data
**Effort:** 1 hour research + 30 min fix
**Risk:** LOW - Forecast quality not impacted

**Current Issue:**
```
Failed downloads:
- https://api.weather.gov/gridpoints/HNL/12,52/forecast (404 Not Found)
- https://api.weather.gov/gridpoints/HNL/8,65/forecast (404 Not Found)
```

**Tasks:**
- [ ] Research correct NWS API gridpoint coordinates for Oahu
- [ ] Test alternative endpoints:
  - https://api.weather.gov/gridpoints/HFO/...
  - https://api.weather.gov/points/21.3099,-157.8581 (Honolulu)
- [ ] Update config.yaml with working endpoints
- [ ] Verify weather JSON files are created
- [ ] Update weather agent if endpoint structure changed

**Investigation:**
```bash
# Test NWS API endpoints
curl -A "SurfCastAI/1.0" https://api.weather.gov/points/21.3099,-157.8581
# Extract gridpoint URL from response
```

**Status:** DEFERRED - System working fine without weather data

---

### 2.2 Wave Model DODS Timeout
**Status:** ‚ö†Ô∏è External server issue - NOMADS DODS servers timeout
**Priority:** P3 - Image analysis compensates for missing numerical data
**Effort:** 2 hours to implement fallback
**Risk:** LOW - Wave forecast GIFs analyzed successfully

**Current Issue:**
```
Timeouts:
- https://nomads.ncep.noaa.gov:9090/dods/gfs_0p25 (timeout after 30s)
- https://nomads.ncep.noaa.gov:9090/dods/wave_multi_1.glo_0.16 (timeout after 30s)
```

**Tasks:**
- [ ] Increase timeout for DODS endpoints (currently 30s)
- [ ] Implement retry logic with exponential backoff
- [ ] Add fallback to GIF-only analysis (already working)
- [ ] Document that DODS data is optional

**Status:** DEFERRED - GIF analysis via GPT-5-mini provides sufficient data

---

### 2.3 SWAN Model 404 Error
**Status:** ‚ö†Ô∏è Endpoint changed - PacIOOS URL no longer valid
**Priority:** P3 - Not critical for forecasts
**Effort:** 30 minutes
**Risk:** LOW

**Current Issue:**
```
Failed download:
- https://www.pacioos.hawaii.edu/wave-model/swan-oahu/ (404 Not Found)
```

**Tasks:**
- [ ] Find new PacIOOS SWAN model endpoint
- [ ] Check https://www.pacioos.hawaii.edu/voyager/
- [ ] Update config.yaml with correct URL
- [ ] Verify data format hasn't changed

**Status:** DEFERRED - Not critical for production

---

## Priority 3: CODE QUALITY & ROBUSTNESS üü¢

### 3.1 Implement Structured Outputs with Pydantic
**Status:** üí° Enhancement - Current string parsing works
**Priority:** P3 - Nice to have for reliability
**Effort:** 3-4 hours
**Risk:** MEDIUM - Could introduce new bugs

**Rationale:**
Current system generates **natural language forecasts** which is the desired output. Structured outputs are NOT needed for our use case - we want narrative text, not JSON data structures.

**If Implemented (optional):**
- [ ] Create Pydantic models for forecast sections
- [ ] Use OpenAI's structured output mode
- [ ] Add JSON schema validation
- [ ] Ensure backward compatibility with current text format

**Status:** NOT RECOMMENDED - Current text generation is the correct approach

---

### 3.2 Add Circuit Breaker for API Calls
**Status:** üí° Enhancement - Current timeout handling sufficient
**Priority:** P3 - Nice to have for resilience
**Effort:** 2 hours
**Risk:** LOW

**Tasks:**
- [ ] Implement circuit breaker pattern (using pybreaker library)
- [ ] Configure failure thresholds (e.g., 5 failures in 1 minute)
- [ ] Add automatic recovery after cooldown period
- [ ] Log circuit breaker state changes

**Implementation Example:**
```python
from pybreaker import CircuitBreaker

# In ForecastEngine.__init__()
self.api_breaker = CircuitBreaker(
    fail_max=5,
    timeout_duration=60,
    name='openai_api'
)

# Wrap API calls
@self.api_breaker
async def _call_openai_api_with_breaker(...):
    return await self._call_openai_api(...)
```

**Status:** DEFERRED - Current error handling is adequate

---

### 3.3 Implement Model Fallback Mechanism
**Status:** üí° Enhancement - Current model very reliable
**Priority:** P3 - Nice to have for redundancy
**Effort:** 1 hour
**Risk:** LOW

**Tasks:**
- [ ] Define fallback model hierarchy:
  - Primary: gpt-5-mini (image analysis), gpt-5-nano (text)
  - Fallback: gpt-4o-mini, gpt-4o
- [ ] Implement automatic fallback on persistent failures
- [ ] Log fallback events
- [ ] Track cost differences

**Implementation:**
```python
# In config.yaml
openai:
  primary_model: gpt-5-mini
  fallback_models:
    - gpt-5-nano
    - gpt-4o-mini
    - gpt-4o

# In forecast_engine.py
async def _call_api_with_fallback(self, ...):
    models = [self.primary_model] + self.fallback_models
    for model in models:
        try:
            return await self._call_openai_api(model=model, ...)
        except Exception as e:
            self.logger.warning(f"Model {model} failed: {e}")
            continue
    raise Exception("All models failed")
```

**Status:** DEFERRED - GPT-5-mini has 100% success rate in testing

---

### 3.4 Move Hardcoded Values to Configuration
**Status:** ‚úÖ ALREADY DONE - Most values in config.yaml
**Priority:** P3 - Verify completeness
**Effort:** 30 minutes audit
**Risk:** NONE

**Audit Checklist:**
- [x] Model names ‚Üí config.yaml ‚úÖ
- [x] Rate limits ‚Üí config.yaml ‚úÖ
- [x] Data source URLs ‚Üí config.yaml ‚úÖ
- [x] Output directory ‚Üí config.yaml ‚úÖ
- [x] Timeout values ‚Üí config.yaml ‚úÖ
- [ ] Image detail levels ‚Üí Currently hardcoded in forecast_engine.py
- [ ] Debug directory path ‚Üí Currently hardcoded
- [ ] Token estimation values ‚Üí Currently hardcoded (3000, 1500, 500)

**Remaining Hardcoded Values:**
```python
# forecast_engine.py line 506-509
if detail == 'high':
    estimated_tokens += 3000
elif detail == 'auto':
    estimated_tokens += 1500
elif detail == 'low':
    estimated_tokens += 500
```

**Tasks:**
- [ ] Move token estimates to config.yaml
- [ ] Move image detail defaults to config
- [ ] Move debug directory pattern to config

**Status:** LOW PRIORITY - Current hardcoded values are sensible defaults

---

## Priority 4: MONITORING & PRODUCTION READINESS üü¢

### 4.1 Implement Health Check Endpoint
**Status:** üí° Enhancement - Needed for production monitoring
**Priority:** P2 - Important for deployment
**Effort:** 1 hour
**Risk:** LOW

**Tasks:**
- [ ] Add health check route to web viewer (FastAPI)
- [ ] Check data collection success rate
- [ ] Check forecast generation status
- [ ] Check disk space in data/output directories
- [ ] Return JSON status with HTTP 200/500

**Implementation:**
```python
# In src/web/viewer.py
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "last_forecast": get_latest_forecast_id(),
        "disk_space_gb": get_disk_space(),
        "data_collection_rate": get_success_rate()
    }
```

**Status:** RECOMMENDED for production deployment

---

### 4.2 Add Cost Alert Thresholds
**Status:** üí° Enhancement - Prevent cost overruns
**Priority:** P2 - Important for budget management
**Effort:** 30 minutes
**Risk:** LOW

**Tasks:**
- [ ] Define cost thresholds in config:
  - Warning: $0.02/forecast
  - Critical: $0.05/forecast
- [ ] Log warnings when thresholds exceeded
- [ ] Optional: Send email/Slack alerts
- [ ] Track daily/monthly totals

**Implementation:**
```yaml
# config.yaml
monitoring:
  cost_thresholds:
    warning: 0.02  # USD per forecast
    critical: 0.05
    daily_limit: 1.00
    monthly_limit: 30.00
```

**Status:** RECOMMENDED for production deployment

---

### 4.3 Add Error Rate Monitoring
**Status:** üí° Enhancement - Track reliability
**Priority:** P2 - Important for SLA monitoring
**Effort:** 1 hour
**Risk:** LOW

**Tasks:**
- [ ] Track success/failure rates by component:
  - Data collection (per source)
  - Image analysis (per type)
  - Forecast generation (per section)
- [ ] Calculate rolling 24-hour success rates
- [ ] Alert on degraded performance (<80% success)
- [ ] Dashboard visualization

**Status:** RECOMMENDED for production deployment

---

### 4.4 Implement Comprehensive Logging
**Status:** ‚úÖ ALREADY DONE - Excellent logging in place
**Priority:** P3 - Verify completeness
**Effort:** 30 minutes audit
**Risk:** NONE

**Current Logging:**
- ‚úÖ Data collection progress
- ‚úÖ API call timing
- ‚úÖ Image analysis completion
- ‚úÖ Forecast generation steps
- ‚úÖ Error handling with stack traces
- ‚úÖ Debug file creation

**Improvements:**
- [ ] Add structured logging (JSON format)
- [ ] Add correlation IDs (bundle_id already used)
- [ ] Add request/response logging for API calls
- [ ] Rotate log files by size/date

**Status:** CURRENT LOGGING IS EXCELLENT - Enhancements optional

---

## Priority 5: DOCUMENTATION IMPROVEMENTS üü¢

### 5.1 Update Architecture Documentation
**Status:** ‚ö†Ô∏è Needs enhancement
**Priority:** P2 - Important for maintainability
**Effort:** 2 hours
**Risk:** NONE

**Tasks:**
- [ ] Create ARCHITECTURE.md with:
  - System overview diagram
  - Data flow diagrams
  - Component descriptions
  - API integration patterns
- [ ] Document GPT-5 vision integration
- [ ] Document error handling strategy
- [ ] Document cost optimization techniques

**Status:** RECOMMENDED before team handoff

---

### 5.2 Create Deployment Guide
**Status:** ‚ö†Ô∏è Missing
**Priority:** P2 - Needed for production
**Effort:** 1 hour
**Risk:** NONE

**Tasks:**
- [ ] Create DEPLOYMENT.md with:
  - Server requirements
  - Environment setup
  - Cron job configuration
  - Monitoring setup
- [ ] Document backup/recovery procedures
- [ ] Document scaling considerations
- [ ] Create deployment checklist

**Status:** REQUIRED before production deployment

---

### 5.3 Enhance API Documentation
**Status:** ‚ö†Ô∏è Needs improvement
**Priority:** P3 - Nice to have
**Effort:** 1 hour
**Risk:** NONE

**Tasks:**
- [ ] Document all configuration options
- [ ] Add code examples for common tasks
- [ ] Document data models (SwellEvent, ForecastLocation, etc.)
- [ ] Create API reference for forecast_engine

**Status:** RECOMMENDED for maintainability

---

## NOT RECOMMENDED - Already Working Correctly ‚ùå

### ‚ùå Switch to "More Reliable Model"
**Status:** ‚úÖ ALREADY USING RECOMMENDED MODELS
**Reason:** We're already using GPT-5-mini and GPT-5-nano
**Evidence:** 100% success rate in live testing, $0.0066/forecast cost

The current models are:
- GPT-5-mini for image analysis (recommended by OpenAI for vision tasks)
- GPT-5-nano for text generation (cost-efficient, perfect quality)
- Both models had 100% success rate in production testing

**NO ACTION NEEDED**

---

### ‚ùå Fix Bugs in visualization.py
**Status:** ‚úÖ ALL BUGS ALREADY FIXED
**Reason:** All visualization bugs resolved in previous debugging session
**Evidence:**
- degrees_to_cardinal() helper added
- Field access corrected (primary_components[0]["period"])
- Null checks added
- Try/except wrappers in place

**NO ACTION NEEDED**

---

### ‚ùå Implement Structured Outputs Using Pydantic
**Status:** NOT NEEDED - Natural language output is correct
**Reason:** Forecast output should be narrative text, not JSON
**Evidence:** Professional-quality markdown/HTML/PDF forecasts generated

Our system generates **natural language surf forecasts** in Pat Caldwell style. Structured outputs would force the model to output JSON instead of narrative text, which would reduce quality.

**NO ACTION NEEDED - Current approach is correct**

---

## Summary: What Actually Needs to Be Done

### CRITICAL (Do Before Production) üî¥
1. **Fix API key security** (15 min) - MUST DO
2. **Add token usage tracking** (30 min) - MUST DO

### IMPORTANT (Do For Production) üü°
3. **Add health check endpoint** (1 hour)
4. **Add cost alerting** (30 min)
5. **Create deployment guide** (1 hour)

### NICE TO HAVE (Do After Stable) üü¢
6. **Add circuit breaker** (2 hours)
7. **Add monitoring dashboard** (3 hours)
8. **Create architecture docs** (2 hours)

### NOT NEEDED ‚ùå
- ‚ùå Switch models (already using recommended ones)
- ‚ùå Fix visualization bugs (already fixed)
- ‚ùå Structured outputs (wrong approach for our use case)
- ‚ùå Fix weather/wave agents (external API issues, not our bugs)

---

## Implementation Timeline

### Week 1: Critical Fixes
- Day 1: Fix API key security (15 min)
- Day 1: Add token usage tracking (30 min)
- Day 2: Test both changes in production

### Week 2: Production Readiness
- Day 1: Add health check endpoint (1 hour)
- Day 2: Add cost alerting (30 min)
- Day 3: Create deployment guide (1 hour)
- Day 4: Full production testing

### Week 3+: Enhancements (Optional)
- Circuit breaker implementation
- Monitoring dashboard
- Architecture documentation

---

## Success Metrics

After implementing Priority 1-2 items:
- [ ] API key secure (not in version control)
- [ ] Actual costs tracked (not estimated)
- [ ] Health checks passing
- [ ] Cost alerts configured
- [ ] Deployment guide complete
- [ ] System deployed to production
- [ ] Monitoring active

**Total Critical Effort:** 45 minutes (API key + cost tracking)
**Total Production Effort:** 3 hours (health check + alerting + docs)

---

*TODO created: October 4, 2025*
*Based on: FINAL_LIVE_TEST_REPORT.md + LIBRARY_REVIEW_REPORT.md*
*Status: System is production-ready after 45 minutes of critical fixes*
