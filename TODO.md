# SurfCastAI TODO - Comprehensive Project Roadmap

**Date:** October 14, 2025
**Based On:** 5-Phase Comprehensive Project Review
**Status:** Production-ready core with critical fixes needed
**Review Reports:** See DOCUMENTATION_AUDIT_REPORT.md, SECURITY_AUDIT_REPORT.md, TEST_COVERAGE_ANALYSIS_REPORT.md

---

## Executive Summary

**Current State:**
- ‚úÖ Core forecasting: PRODUCTION-READY (Quick Wins active, specialist integration 70% complete)
- ‚ö†Ô∏è Security: 3 CRITICAL ISSUES (fixable in 1 hour)
- ‚ö†Ô∏è Tests: 35 FAILING (96.6% pass rate, but failures mask regressions)
- ‚ö†Ô∏è Documentation: OUTDATED (claims 90 tests, actual 1,017; claims 93% coverage, actual 66%)
- ‚ö†Ô∏è Deployment: 75% READY (manual deployment works, needs containerization)

**Path to Production:**
1. **Phase 0** (1 hour): Fix critical security issues ‚Üí SAFE TO DEPLOY
2. **Phase 1** (1-2 days): Stabilize tests + docs ‚Üí RELIABLE
3. **Phase 2** (2-3 days): Create deployment artifacts ‚Üí SCALABLE
4. **Phase 3** (1-2 weeks): Improve test coverage ‚Üí MAINTAINABLE
5. **Phase 4** (OPTIONAL): Pydantic migration + refactoring ‚Üí EXCELLENT

---

## Decision Tree: Choose Your Deployment Path

**üöÄ Want to deploy NOW with minimal changes?**
‚Üí Complete **Phase 0 only** (1 hour) ‚Üí System is **SAFE TO DEPLOY**

**‚úÖ Want production-grade stability?**
‚Üí Complete **Phase 0 + Phase 1** (1.5 days) ‚Üí System is **STABLE & DOCUMENTED**

**üê≥ Want containerized deployment?**
‚Üí Complete **Phase 0 + Phase 1 + Phase 2** (4 days) ‚Üí System is **SCALABLE**

**üìä Want comprehensive test coverage?**
‚Üí Complete **all phases** (2-3 weeks) ‚Üí System is **WELL-TESTED**

**Most users should complete Phase 0, deploy to production, then iterate on Phases 1-3 as time allows.**

---

## QUICK START: Deploy in 1 Hour ‚ö°

If you need to deploy to production **immediately**, complete **ONLY Phase 0**:

### Phase 0 Quick Checklist (4 tasks, 40 minutes)

1. **Rotate API Key** (15 min):
   ```bash
   # Visit https://platform.openai.com/api-keys
   # Revoke old key, generate new key, update .env
   chmod 600 .env
   ```

2. **Unstage config.yaml** (5 min):
   ```bash
   git reset HEAD config/config.yaml
   ```

3. **Fix Dependencies** (10 min):
   ```bash
   pip install --upgrade -r requirements.txt
   ```

4. **Set Permissions** (10 min):
   ```bash
   chmod 600 .env config/config.yaml data/validation.db
   chmod 700 config data
   ```

**‚úÖ After Phase 0: System is SAFE TO DEPLOY**
- Security score: 7.5/10 ‚Üí 8.5/10
- Manual deployment ready (see DEPLOYMENT.md)
- Phases 1-3 are **optional post-production improvements**

See detailed Phase 0 instructions below ‚¨áÔ∏è

---

## Phase 0: CRITICAL SECURITY FIXES (1 hour, BLOCKING) üö®

**MUST BE COMPLETED BEFORE ANY DEPLOYMENT**

### Task 0.1: Rotate Exposed OpenAI API Key

**Agent:** security-auditor
**Priority:** CRITICAL
**Effort:** 15 minutes
**Blocking:** ALL deployment activities

**Problem:**
Security audit found exposed API key in `.env` file (file permissions too open).

**Instructions:**

1. **Rotate API key immediately:**
   - Visit https://platform.openai.com/api-keys
   - Revoke current key: `sk-proj-...` (visible in `.env`)
   - Generate new key
   - Copy new key

2. **Update .env file:**
   ```bash
   # Backup old .env
   cp .env .env.backup.old

   # Update with new key
   nano .env
   # Replace OPENAI_API_KEY=sk-proj-OLD with OPENAI_API_KEY=sk-proj-NEW

   # Set restrictive permissions
   chmod 600 .env
   ```

3. **Verify:**
   ```bash
   # Check permissions
   ls -la .env
   # Should show: -rw------- (600)

   # Test new key
   python src/main.py validate-config
   # Should succeed without errors
   ```

**Acceptance Criteria:**
- ‚úÖ Old API key revoked in OpenAI dashboard
- ‚úÖ New API key in `.env` file
- ‚úÖ `.env` permissions set to 600 (-rw-------)
- ‚úÖ Config validation passes

**Verification:**
```bash
# Verify file not readable by others
ls -la .env | grep "rw-------"

# Test API key works
python -c "import openai; print('API key valid')"
```

---

### Task 0.2: Unstage config.yaml from Git

**Agent:** security-auditor
**Priority:** CRITICAL
**Effort:** 5 minutes
**Blocking:** ALL commits

**Problem:**
`config/config.yaml` shows as modified in `git status` and may contain sensitive data.

**Instructions:**

1. **Unstage file:**
   ```bash
   git reset HEAD config/config.yaml
   ```

2. **Verify gitignore:**
   ```bash
   git check-ignore -v config/config.yaml
   # Expected output: config/config.yaml (from .gitignore line 3)
   ```

3. **Check for leaked data in history:**
   ```bash
   # Search git history for API keys
   git log --all --full-history -- config/config.yaml

   # If file was committed with sensitive data:
   # WARNING: This rewrites history, coordinate with team
   git filter-branch --force --index-filter \
     'git rm --cached --ignore-unmatch config/config.yaml' \
     --prune-empty --tag-name-filter cat -- --all
   ```

**Acceptance Criteria:**
- ‚úÖ `config/config.yaml` not shown in `git status`
- ‚úÖ `.gitignore` contains `config/config.yaml` exclusion
- ‚úÖ No sensitive data in git history

**Verification:**
```bash
# Should show config.yaml is ignored
git status | grep -v "config.yaml"

# Verify gitignore working
echo "test" >> config/config.yaml
git status | grep -v "config.yaml"
git checkout config/config.yaml  # Undo test change
```

---

### Task 0.3: Fix Dependency Version Mismatch

**Agent:** security-auditor
**Priority:** CRITICAL
**Effort:** 10 minutes
**Blocking:** Production deployment

**Problem:**
Pillow downgraded from 11.0.0 ‚Üí 10.4.0, missing security patches.

**Instructions:**

1. **Check current versions:**
   ```bash
   pip list | grep -E "(Pillow|aiohttp|openai|pydantic)"
   ```

2. **Upgrade to pinned versions:**
   ```bash
   pip install --upgrade -r requirements.txt
   ```

3. **Verify installed versions match requirements.txt:**
   ```bash
   pip list | grep Pillow
   # Expected: Pillow 11.0.0

   pip list | grep aiohttp
   # Expected: aiohttp 3.12.11
   ```

4. **Test application:**
   ```bash
   # Quick smoke test
   python -c "from PIL import Image; print('Pillow OK')"
   python src/main.py validate-config
   ```

**Acceptance Criteria:**
- ‚úÖ All packages match requirements.txt versions
- ‚úÖ Pillow == 11.0.0 (not 10.4.0)
- ‚úÖ aiohttp == 3.12.11
- ‚úÖ Application runs without import errors

**Verification:**
```bash
# Generate frozen requirements and compare
pip freeze > /tmp/installed.txt
diff <(grep -E "Pillow|aiohttp|openai|pydantic" requirements.txt | sort) \
     <(grep -E "Pillow|aiohttp|openai|pydantic" /tmp/installed.txt | sort)
# Should show no differences
```

---

### Task 0.4: Set Proper File Permissions

**Agent:** security-auditor
**Priority:** HIGH
**Effort:** 10 minutes
**Blocking:** Production deployment

**Problem:**
Sensitive files may have overly permissive file permissions.

**Instructions:**

1. **Set restrictive permissions on sensitive files:**
   ```bash
   # Configuration files (only owner can read/write)
   chmod 600 .env
   chmod 600 config/config.yaml

   # Database (only owner can read/write)
   chmod 600 data/validation.db

   # Directories (owner can read/write/execute)
   chmod 700 config
   chmod 700 data
   ```

2. **Verify permissions:**
   ```bash
   ls -la .env config/config.yaml data/validation.db
   # All should show: -rw------- (600)

   ls -ld config data
   # Both should show: drwx------ (700)
   ```

3. **Document in README:**
   - Add section on file permissions
   - Add to deployment checklist

**Acceptance Criteria:**
- ‚úÖ `.env` is 600 (-rw-------)
- ‚úÖ `config/config.yaml` is 600
- ‚úÖ `data/validation.db` is 600
- ‚úÖ `config/` directory is 700 (drwx------)
- ‚úÖ `data/` directory is 700

**Verification:**
```bash
# Automated check
find . -name ".env" -o -name "config.yaml" -o -name "validation.db" | \
while read file; do
  perm=$(stat -f "%OLp" "$file" 2>/dev/null || stat -c "%a" "$file" 2>/dev/null)
  if [ "$perm" != "600" ]; then
    echo "FAIL: $file has permissions $perm (expected 600)"
  else
    echo "PASS: $file"
  fi
done
```

---

**PHASE 0 COMPLETION CHECKLIST:**
- [ ] Task 0.1: API key rotated and .env secured (600 permissions)
- [ ] Task 0.2: config.yaml unstaged and gitignored
- [ ] Task 0.3: All dependencies upgraded to match requirements.txt
- [ ] Task 0.4: Sensitive files have restrictive permissions (600/700)
- [ ] Verification: `python src/main.py validate-config` passes
- [ ] Verification: `git status` shows no sensitive files staged

**AFTER PHASE 0:** System is SAFE TO DEPLOY (security score: 7.5/10 ‚Üí 8.5/10)

---

## Phase 1: STABILIZATION (OPTIONAL - Post-Production) ‚ö°

**Priority:** HIGH (but not blocking deployment)
**Timeline:** 1-2 days
**Goal:** Fix failing tests, update outdated documentation, establish monitoring

**Note:** You can deploy after Phase 0 and complete Phase 1 later. Current test pass rate (96.6%) is acceptable for production, and documentation gaps don't affect functionality. This phase improves reliability and maintainability but is not a deployment blocker.

### Task 1.1: Fix 19 Outdated Test Assertions

**Agent:** test-guardian
**Priority:** HIGH
**Effort:** 2-3 hours
**Blocking:** Test suite reliability

**Problem:**
19 tests fail due to outdated assertions after code refactoring (not actual bugs).

**Test Failures Breakdown:**
- 6 security tests (test_ssrf_protection.py, test_secure_extraction.py)
- 5 config tests (test_config_validation.py)
- 3 data fusion tests (test_hawaiian_scale.py)
- 1 wave model test (same Hawaiian scale formula)
- 4 HTTP client tests (async mock setup)

**Instructions:**

1. **Fix security test assertions:**
   ```bash
   # File: tests/unit/utils/test_ssrf_protection.py
   # Lines with failures: Check recent git history
   git diff HEAD~5 src/utils/security.py

   # Update assertions to match new behavior
   # Agent: Review failures and update assertions
   ```

2. **Fix config test assertions:**
   ```bash
   # File: tests/unit/core/test_config_validation.py
   # Update validation error messages to match current config.py
   ```

3. **Fix Hawaiian scale formula tests:**
   ```bash
   # Files:
   #   tests/unit/processing/test_data_fusion_system.py
   #   tests/unit/processing/test_wave_model.py
   # Update expected values for new formula
   ```

4. **Fix HTTP client async mocks:**
   ```bash
   # File: tests/unit/core/test_http_client.py
   # Fix mock setup for aiohttp 3.12.11
   # See: https://docs.aiohttp.org/en/stable/testing.html
   ```

**Detailed Instructions Per Test File:**

```python
# tests/unit/utils/test_ssrf_protection.py
# CHANGE: Update blocked IP assertions
# OLD: assert validate_url("http://192.168.1.1") raises ValueError
# NEW: assert validate_url("http://192.168.1.1") raises SSRFProtectionError

# tests/unit/core/test_config_validation.py
# CHANGE: Update error message assertions
# OLD: assert "Invalid API key" in errors
# NEW: assert "API key must start with 'sk-'" in errors

# tests/unit/processing/test_data_fusion_system.py
# CHANGE: Update Hawaiian scale values
# OLD: assert hawaiian_scale == 1.5
# NEW: assert hawaiian_scale == pytest.approx(1.8, rel=0.1)

# tests/unit/core/test_http_client.py
# CHANGE: Fix async mock setup
# OLD: @pytest.fixture\ndef mock_session():\n    return MagicMock()
# NEW: @pytest.fixture\nasync def mock_session():\n    session = AsyncMock()\n    return session
```

**Acceptance Criteria:**
- ‚úÖ All 19 assertion tests pass
- ‚úÖ No changes to production code (only test assertions)
- ‚úÖ Test coverage maintained (still 66%)

**Verification:**
```bash
# Run specific test files
pytest tests/unit/utils/test_ssrf_protection.py -v
pytest tests/unit/core/test_config_validation.py -v
pytest tests/unit/processing/test_data_fusion_system.py -v
pytest tests/unit/core/test_http_client.py -v

# Verify pass rate improved
pytest tests/ --tb=short | grep "passed"
# Expected: ~1,001 passed (was 982 before)
```

---

### Task 1.2: Regenerate 12 Stale Golden Snapshots

**Agent:** test-guardian
**Priority:** HIGH
**Effort:** 30 minutes
**Blocking:** Prompt testing reliability

**Problem:**
12 golden snapshot tests fail because SeniorForecaster output format changed.

**Instructions:**

1. **Understand golden snapshot testing:**
   ```bash
   # Golden snapshots store expected LLM outputs
   # Located in: tests/prompt_tests/golden_snapshots/
   ls tests/prompt_tests/golden_snapshots/
   ```

2. **Regenerate snapshots:**
   ```bash
   # Run with UPDATE flag to regenerate
   pytest tests/prompt_tests/ --update-snapshots -v

   # Or manually delete old snapshots
   rm -rf tests/prompt_tests/golden_snapshots/*.json
   pytest tests/prompt_tests/ -v
   # Will generate new snapshots
   ```

3. **Review changes:**
   ```bash
   git diff tests/prompt_tests/golden_snapshots/
   # Verify changes are expected (format changes, not content degradation)
   ```

4. **Commit new snapshots:**
   ```bash
   git add tests/prompt_tests/golden_snapshots/
   git commit -m "chore: regenerate golden snapshots after SeniorForecaster format update"
   ```

**Acceptance Criteria:**
- ‚úÖ All 12 golden snapshot tests pass
- ‚úÖ New snapshots reflect current SeniorForecaster output format
- ‚úÖ No degradation in output quality (manual review required)

**Verification:**
```bash
# Run prompt tests
pytest tests/prompt_tests/ -v --tb=short
# Expected: 12/12 passing (was 0/12 before)

# Overall test status
pytest tests/ --tb=short | tail -1
# Expected: ~1,013 passed, 4 failed (was 982 passed, 35 failed)
```

---

### Task 1.3: Update CLAUDE.md Documentation

**Agent:** search-specialist
**Priority:** HIGH
**Effort:** 1 hour
**Blocking:** Developer onboarding

**Problem:**
CLAUDE.md contains outdated metrics and contradictory status claims.

**Specific Fixes Needed:**

1. **Fix test metrics (lines 8-9):**
   ```markdown
   # OLD:
   **Quick Wins implementation COMPLETE (3/3 improvements) - system now exceeds human forecasting abilities**
   - 90 tests, 100% passing
   - 93% test coverage

   # NEW:
   **Quick Wins implementation COMPLETE (3/3 improvements) - system now exceeds human forecasting abilities**
   - 1,017 tests total (982 passing, 35 failing = 96.6% pass rate)
   - 66% test coverage (target: 90%)
   - Quick Wins features: 98 tests, 99% passing, 91% avg coverage
   ```

2. **Clarify Pydantic migration status (lines 10-15):**
   ```markdown
   # OLD:
   Phase 1 Pydantic data contracts 70% complete (7/10 tasks)

   # NEW:
   Phase 1 Specialist Pydantic Integration: 70% complete (7/10 tasks)
   - Specialist schemas (BuoyAnalyst, PressureAnalyst, SeniorForecaster): ‚úÖ 100% Pydantic
   - Core data models (BuoyData, SwellEvent, WeatherData): ‚ùå Still dataclasses
   - Overall codebase Pydantic adoption: ~5% (specialists only)
   - Remaining work: Tasks 9-10 (specialist unit test updates, full verification)
   ```

3. **Update status date (line 1):**
   ```markdown
   # OLD:
   *Last Updated: October 11, 2025*

   # NEW:
   *Last Updated: October 14, 2025*
   ```

4. **Add auto-collection status (new section after line 20):**
   ```markdown
   ### Auto-Collection Feature (COMPLETED October 13, 2025)
   - **Status:** ‚úÖ IMPLEMENTED in src/main.py lines 783, 867-878
   - **Behavior:** `python src/main.py run --mode forecast` auto-collects fresh data
   - **Override:** Use `--skip-collection` flag to use existing bundle
   - **Impact:** Eliminates stale data forecasts (was causing 2-day-old data issues)
   ```

5. **Fix Quick Wins line counts (lines 85-87):**
   ```markdown
   # OLD:
   - Storm detector: 476 lines, 28 tests

   # NEW:
   - Storm detector: 568 lines (19% over estimate), 30 tests, 84% coverage
   ```

6. **Add missing files note:**
   ```markdown
   ### Missing Documentation Files
   The following files referenced in CLAUDE.md were archived during Oct 14 cleanup:
   - GEM_ROADMAP.md ‚Üí ARCHIVE_20251013/root_files/docs/
   - QUICK_WINS_COMPLETE.md ‚Üí ARCHIVE_20251013/root_files/docs/
   - See CLEANUP_REPORT.md for complete archive manifest
   ```

**Acceptance Criteria:**
- ‚úÖ All metrics updated to reflect actual current state
- ‚úÖ Pydantic migration clarified (specialist 70%, codebase 5%)
- ‚úÖ Auto-collection marked COMPLETE
- ‚úÖ Line counts corrected
- ‚úÖ Date updated to Oct 14, 2025

**Verification:**
```bash
# Verify changes
git diff CLAUDE.md

# Check no remaining "90 tests" references
grep -n "90 tests" CLAUDE.md
# Expected: No matches

# Check updated date
head -1 CLAUDE.md | grep "October 14, 2025"
# Expected: Match found
```

---

### Task 1.4: Update TODO.md (This File)

**Agent:** N/A (manual update after Phase 0-1 completion)
**Priority:** HIGH
**Effort:** 10 minutes (already done by creating this document)
**Blocking:** Project roadmap clarity

**Instructions:**
This task is implicitly completed by creating this comprehensive TODO.md document.

**Acceptance Criteria:**
- ‚úÖ Old TODO.md (Oct 13, validation-focused) replaced with this comprehensive version
- ‚úÖ All review findings incorporated
- ‚úÖ Detailed task instructions provided
- ‚úÖ Agent assignments specified
- ‚úÖ Acceptance criteria defined for every task

---

### Task 1.5: Update README.md Command Examples

**Agent:** search-specialist
**Priority:** MEDIUM
**Effort:** 30 minutes
**Blocking:** User onboarding

**Problem:**
README.md line 357 references non-existent `python surfcast.py` command.

**Instructions:**

1. **Find and fix broken command:**
   ```bash
   # Locate the issue
   grep -n "surfcast.py" README.md
   # Expected: Line 357

   # Replace with correct command
   sed -i '' 's/python surfcast.py/python src\/main.py/g' README.md
   ```

2. **Review all command examples:**
   ```bash
   # Extract all commands from README
   grep -E "^\s*\$|^\s*python|^\s*pytest" README.md

   # Verify each command is valid
   # Check against src/main.py argument parser
   ```

3. **Test documented commands:**
   ```bash
   # Test each command from README
   python src/main.py --help
   python src/main.py run --mode collect
   python src/main.py list
   python src/main.py info
   ```

**Commands to Update:**

```markdown
# Line 357 - Interactive UI
# OLD: python surfcast.py
# NEW: python scripts/surf_launcher.py

# Add note about launcher
The interactive cyberpunk UI is available via:
`python scripts/surf_launcher.py`

# Verify other examples
Line 120: python src/main.py run --mode forecast  ‚úÖ CORRECT
Line 145: python src/main.py list  ‚úÖ CORRECT
Line 160: python src/main.py info --bundle BUNDLE_ID  ‚úÖ CORRECT
```

**Acceptance Criteria:**
- ‚úÖ All command examples use correct file paths
- ‚úÖ No references to non-existent `surfcast.py`
- ‚úÖ Interactive launcher correctly documented
- ‚úÖ All documented commands tested and working

**Verification:**
```bash
# Verify no broken commands remain
grep -n "surfcast.py" README.md
# Expected: No matches

# Test launcher exists
ls scripts/surf_launcher.py
# Expected: File found
```

---

### Task 1.6: Create Backup Automation Script

**Agent:** deployment-engineer
**Priority:** HIGH
**Effort:** 1 hour
**Blocking:** Data safety in production

**Problem:**
No automated backup script exists (only documented in DEPLOYMENT.md).

**Instructions:**

1. **Create scripts/backup_database.sh:**

```bash
#!/bin/bash
# SurfCastAI Database Backup Script
# Based on DEPLOYMENT.md lines 555-610

set -euo pipefail

# Configuration
BACKUP_DIR="${BACKUP_DIR:-/opt/surfCastAI/backups}"
DB_PATH="${DB_PATH:-/opt/surfCastAI/data/validation.db}"
RETENTION_DAYS="${RETENTION_DAYS:-30}"
S3_BUCKET="${S3_BUCKET:-}"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="validation_db_${TIMESTAMP}.db"

# Create backup directory
mkdir -p "$BACKUP_DIR"

# Backup database with SQLite integrity check
echo "[$(date)] Starting backup: $BACKUP_FILE"

if [ ! -f "$DB_PATH" ]; then
    echo "ERROR: Database not found at $DB_PATH"
    exit 1
fi

# Perform backup using SQLite backup API (handles WAL mode)
sqlite3 "$DB_PATH" ".backup '$BACKUP_DIR/$BACKUP_FILE'"

# Verify backup integrity
sqlite3 "$BACKUP_DIR/$BACKUP_FILE" "PRAGMA integrity_check;" | grep "ok" > /dev/null
if [ $? -eq 0 ]; then
    echo "[$(date)] Backup successful: $BACKUP_FILE"

    # Compress backup
    gzip "$BACKUP_DIR/$BACKUP_FILE"
    echo "[$(date)] Compressed: ${BACKUP_FILE}.gz"

    # Upload to S3 if configured
    if [ -n "$S3_BUCKET" ]; then
        aws s3 cp "$BACKUP_DIR/${BACKUP_FILE}.gz" \
            "s3://$S3_BUCKET/backups/database/${BACKUP_FILE}.gz"
        echo "[$(date)] Uploaded to S3: s3://$S3_BUCKET/backups/database/"
    fi

    # Cleanup old backups
    find "$BACKUP_DIR" -name "validation_db_*.db.gz" -mtime +${RETENTION_DAYS} -delete
    echo "[$(date)] Cleaned up backups older than ${RETENTION_DAYS} days"

else
    echo "ERROR: Backup integrity check failed"
    exit 1
fi

echo "[$(date)] Backup complete"
```

2. **Make executable:**
   ```bash
   chmod +x scripts/backup_database.sh
   ```

3. **Test backup script:**
   ```bash
   # Dry run
   BACKUP_DIR=/tmp/test_backups \
   DB_PATH=data/validation.db \
   ./scripts/backup_database.sh

   # Verify backup created
   ls -lh /tmp/test_backups/
   # Should show: validation_db_YYYYMMDD_HHMMSS.db.gz

   # Test restore
   gunzip -c /tmp/test_backups/validation_db_*.db.gz > /tmp/restored.db
   sqlite3 /tmp/restored.db "PRAGMA integrity_check;"
   # Expected: ok

   # Cleanup test
   rm -rf /tmp/test_backups /tmp/restored.db
   ```

4. **Add to crontab:**
   ```bash
   # Add to deployment/cron_example.sh or create new crontab entry
   # Run daily at 2 AM local time
   0 2 * * * /opt/surfCastAI/scripts/backup_database.sh >> /opt/surfCastAI/logs/backup.log 2>&1
   ```

5. **Document in DEPLOYMENT.md:**
   - Update lines 555-610 to reference actual script
   - Add restoration instructions

**Acceptance Criteria:**
- ‚úÖ `scripts/backup_database.sh` created and executable
- ‚úÖ Script performs SQLite backup using .backup command (handles WAL)
- ‚úÖ Script compresses backup with gzip
- ‚úÖ Script verifies backup integrity
- ‚úÖ Script cleans up old backups (configurable retention)
- ‚úÖ Script supports S3 upload (optional)
- ‚úÖ Script tested with validation.db
- ‚úÖ Crontab entry documented

**Verification:**
```bash
# Test script exists and is executable
test -x scripts/backup_database.sh && echo "PASS" || echo "FAIL"

# Test backup creation
./scripts/backup_database.sh
ls -lh backups/validation_db_*.db.gz
# Expected: File created

# Test integrity
gunzip -c backups/validation_db_*.db.gz > /tmp/test.db
sqlite3 /tmp/test.db "SELECT COUNT(*) FROM validation_results;"
# Expected: Row count matches original database
rm /tmp/test.db
```

---

### Task 1.7: Setup Health Monitoring

**Agent:** deployment-engineer
**Priority:** HIGH
**Effort:** 30 minutes
**Blocking:** Production reliability

**Problem:**
No external health monitoring configured (risk of silent failures).

**Instructions:**

1. **Sign up for Healthchecks.io (free tier):**
   - Visit https://healthchecks.io
   - Create account
   - Create new check: "SurfCastAI Forecast"
   - Set schedule: "Daily at 6 AM HST"
   - Set grace period: 2 hours
   - Copy ping URL

2. **Update deployment/cron_example.sh to ping healthcheck:**

```bash
#!/bin/bash
# SurfCastAI Cron Script with Health Monitoring

LOG_DIR="/opt/surfCastAI/logs"
LOG_FILE="$LOG_DIR/cron-$(date +%Y%m%d).log"
HEALTHCHECK_URL="${HEALTHCHECK_URL:-}"  # Set in environment

# Create log directory
mkdir -p "$LOG_DIR"

echo "=== SurfCastAI Forecast Run: $(date) ===" >> "$LOG_FILE"

# Run forecast
cd /opt/surfCastAI
source venv/bin/activate

python src/main.py run --mode full >> "$LOG_FILE" 2>&1
EXIT_CODE=$?

# Ping healthcheck
if [ $EXIT_CODE -eq 0 ]; then
    echo "Forecast successful, pinging healthcheck" >> "$LOG_FILE"
    if [ -n "$HEALTHCHECK_URL" ]; then
        curl -fsS -m 10 --retry 5 "$HEALTHCHECK_URL" > /dev/null 2>&1
    fi
else
    echo "Forecast failed with exit code $EXIT_CODE" >> "$LOG_FILE"
    if [ -n "$HEALTHCHECK_URL" ]; then
        curl -fsS -m 10 --retry 5 "${HEALTHCHECK_URL}/fail" > /dev/null 2>&1
    fi
fi

# Cleanup old logs (keep last 7 days)
find "$LOG_DIR" -name "cron-*.log" -mtime +7 -delete

echo "=== Run complete: $(date) ===" >> "$LOG_FILE"
```

3. **Configure systemd service to ping healthcheck:**

Add to `deployment/systemd/surfcastai-forecast.service`:

```ini
[Service]
Environment="HEALTHCHECK_URL=https://hc-ping.com/YOUR-UUID-HERE"
ExecStartPost=/bin/sh -c 'curl -fsS -m 10 --retry 5 $HEALTHCHECK_URL'
ExecStopPost=/bin/sh -c 'curl -fsS -m 10 --retry 5 $HEALTHCHECK_URL/fail'
```

4. **Test health check:**
   ```bash
   # Manual ping
   curl -fsS "https://hc-ping.com/YOUR-UUID-HERE"

   # Check Healthchecks.io dashboard
   # Should show successful ping
   ```

5. **Configure alert notifications:**
   - In Healthchecks.io dashboard
   - Add email notification
   - Add Slack/Discord webhook (optional)
   - Test alert by missing a check

**Acceptance Criteria:**
- ‚úÖ Healthchecks.io account created
- ‚úÖ Check configured for daily forecast runs
- ‚úÖ Cron script pings on success
- ‚úÖ Cron script pings /fail on error
- ‚úÖ systemd service pings on success/failure
- ‚úÖ Alert notifications configured
- ‚úÖ Test ping successful

**Verification:**
```bash
# Test successful ping
curl -fsS "https://hc-ping.com/YOUR-UUID-HERE"
# Check dashboard - should show recent ping

# Test failure ping
curl -fsS "https://hc-ping.com/YOUR-UUID-HERE/fail"
# Check dashboard - should show failure alert

# Test cron script
HEALTHCHECK_URL="https://hc-ping.com/YOUR-UUID-HERE" \
./deployment/cron_example.sh
# Check dashboard - should show ping from cron
```

---

**PHASE 1 COMPLETION CHECKLIST:**
- [ ] Task 1.1: 19 test assertions fixed (pass rate: 96.6% ‚Üí 98.1%)
- [ ] Task 1.2: 12 golden snapshots regenerated (pass rate: 98.1% ‚Üí 99.8%)
- [ ] Task 1.3: CLAUDE.md updated with accurate metrics
- [ ] Task 1.4: TODO.md replaced with this comprehensive version
- [ ] Task 1.5: README.md command examples fixed
- [ ] Task 1.6: Backup automation script created and tested
- [ ] Task 1.7: Health monitoring configured (Healthchecks.io)
- [ ] Verification: `pytest tests/ -v` shows ~1,014 passing, ~3 failing
- [ ] Verification: Documentation accurately reflects system state
- [ ] Verification: Backup script runs successfully
- [ ] Verification: Healthcheck receives successful ping

**AFTER PHASE 1:** System is STABLE and DOCUMENTED (test pass rate: 99.8%, docs accurate)

---

## Phase 2: DEPLOYMENT PREPARATION (OPTIONAL - Adds Containerization) üê≥

**Priority:** MEDIUM (not required for initial deployment)
**Timeline:** 2-3 days
**Goal:** Create deployment artifacts for containerized and systemd deployments

**Note:** Manual deployment already works (see DEPLOYMENT.md). This phase adds Docker and systemd options for easier scaling and management, but you can deploy and run the system successfully without these artifacts. Add containerization when you need it, not before.

### Task 2.1: Create Dockerfile

**Agent:** deployment-engineer
**Priority:** MEDIUM
**Effort:** 2 hours
**Blocking:** Container deployment option

**Problem:**
No Dockerfile exists (only template in DEPLOYMENT.md).

**Instructions:**

1. **Create Dockerfile in repository root:**

```dockerfile
# SurfCastAI Dockerfile
# Multi-stage build for optimized production image

FROM python:3.11-slim AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install Python dependencies
COPY requirements.txt /tmp/
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r /tmp/requirements.txt

# Production stage
FROM python:3.11-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    cairo \
    pango \
    gdk-pixbuf \
    libffi-dev \
    curl \
    sqlite3 \
    && rm -rf /var/lib/apt/lists/*

# Create app user for security
RUN useradd -m -u 1000 -s /bin/bash surfcast && \
    mkdir -p /app /app/data /app/output /app/logs /app/config && \
    chown -R surfcast:surfcast /app

# Copy virtual environment from builder
COPY --from=builder --chown=surfcast:surfcast /opt/venv /opt/venv

# Set environment
ENV PATH="/opt/venv/bin:$PATH" \
    PYTHONUNBUFFERED=1 \
    SURFCAST_DATA_DIR="/app/data" \
    SURFCAST_OUTPUT_DIR="/app/output" \
    SURFCAST_LOG_DIR="/app/logs"

# Set working directory
WORKDIR /app

# Copy application code
COPY --chown=surfcast:surfcast . /app/

# Switch to non-root user
USER surfcast

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose web viewer port
EXPOSE 8000

# Default command (can be overridden)
CMD ["python", "src/main.py", "run", "--mode", "full"]
```

2. **Create .dockerignore:**

```gitignore
# .dockerignore - Exclude from Docker build context

# Version control
.git
.github
.gitignore

# Python cache
__pycache__
*.py[cod]
*.pyo
*.pyd
.Python
*.so
.pytest_cache/
.mypy_cache/
.ruff_cache/
.coverage
htmlcov/

# Virtual environments
venv/
env/
.venv/
ENV/

# Configuration (will be mounted as volumes)
config/config.yaml
.env

# Data directories (will be mounted as volumes)
data/
output/
logs/

# Archives and backups
ARCHIVE_*/
*.db-shm
*.db-wal
backups/

# Documentation
*.md
docs/

# IDE
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Tests
tests/
scripts/
examples/
```

3. **Test Docker build:**
   ```bash
   # Build image
   docker build -t surfcastai:latest .

   # Check image size
   docker images surfcastai:latest
   # Expected: ~800MB (python:3.11-slim base + dependencies)

   # Run container with test config
   docker run --rm \
     -e OPENAI_API_KEY="$OPENAI_API_KEY" \
     -v $(pwd)/data:/app/data \
     -v $(pwd)/output:/app/output \
     -v $(pwd)/config:/app/config \
     surfcastai:latest \
     python src/main.py validate-config

   # Expected: Configuration validation passes
   ```

4. **Test web viewer in container:**
   ```bash
   docker run --rm -d \
     --name surfcastai-web-test \
     -p 8000:8000 \
     -v $(pwd)/output:/app/output:ro \
     surfcastai:latest \
     uvicorn src.web.app:app --host 0.0.0.0 --port 8000

   # Test health endpoint
   curl http://localhost:8000/health
   # Expected: {"status":"ok","timestamp":"...","forecasts":5}

   # Cleanup
   docker stop surfcastai-web-test
   ```

**Acceptance Criteria:**
- ‚úÖ Dockerfile created with multi-stage build
- ‚úÖ .dockerignore created
- ‚úÖ Image builds successfully (<1GB size)
- ‚úÖ Container runs with mounted volumes
- ‚úÖ Health check works
- ‚úÖ Non-root user (surfcast) for security
- ‚úÖ Application functional in container

**Verification:**
```bash
# Build test
docker build -t surfcastai:test .
# Expected: Success

# Run test
docker run --rm -e OPENAI_API_KEY="test" surfcastai:test python --version
# Expected: Python 3.11.x

# Security test (verify non-root)
docker run --rm surfcastai:test whoami
# Expected: surfcast (not root)
```

---

### Task 2.2: Create docker-compose.yml

**Agent:** deployment-engineer
**Priority:** MEDIUM
**Effort:** 1 hour
**Dependencies:** Task 2.1 (Dockerfile)

**Instructions:**

1. **Create docker-compose.yml:**

```yaml
# SurfCastAI Docker Compose Configuration
version: '3.8'

services:
  # Forecast generation service (runs on schedule)
  forecast:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: surfcastai-forecast
    restart: unless-stopped
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./output:/app/output
      - ./logs:/app/logs
      - ./config:/app/config:ro
    environment:
      - SURFCAST_DATA_DIR=/app/data
      - SURFCAST_OUTPUT_DIR=/app/output
      - SURFCAST_LOG_DIR=/app/logs
    command: >
      sh -c "
      while true; do
        python src/main.py run --mode full
        echo 'Forecast complete, sleeping for 6 hours'
        sleep 21600
      done
      "
    networks:
      - surfcastai

  # Web viewer service (always running)
  web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: surfcastai-web
    restart: unless-stopped
    ports:
      - "${WEB_PORT:-8000}:8000"
    volumes:
      - ./output:/app/output:ro
    environment:
      - SURFCAST_OUTPUT_DIR=/app/output
    command: uvicorn src.web.app:app --host 0.0.0.0 --port 8000 --workers 4
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    depends_on:
      - forecast
    networks:
      - surfcastai

networks:
  surfcastai:
    driver: bridge
```

2. **Create .env.example for Docker:**

```bash
# SurfCastAI Environment Variables
# Copy to .env and fill in actual values

# OpenAI API Configuration (REQUIRED)
OPENAI_API_KEY=sk-proj-YOUR-KEY-HERE

# Web Viewer Configuration
WEB_PORT=8000

# Data Directories (usually defaults are fine)
SURFCAST_DATA_DIR=/app/data
SURFCAST_OUTPUT_DIR=/app/output
SURFCAST_LOG_DIR=/app/logs

# Health Check Configuration (optional)
HEALTHCHECK_URL=https://hc-ping.com/YOUR-UUID-HERE
```

3. **Test docker-compose:**
   ```bash
   # Start services
   docker-compose up -d

   # Check services running
   docker-compose ps
   # Expected: Both services "Up"

   # Check web health
   curl http://localhost:8000/health
   # Expected: {"status":"ok",...}

   # Check forecast logs
   docker-compose logs -f forecast
   # Should show forecast generation

   # Stop services
   docker-compose down
   ```

4. **Add to README.md:**

```markdown
## Docker Deployment

### Quick Start

1. **Build and start services:**
   ```bash
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY

   docker-compose up -d
   ```

2. **Access web viewer:**
   ```
   http://localhost:8000
   ```

3. **View logs:**
   ```bash
   docker-compose logs -f
   ```

4. **Stop services:**
   ```bash
   docker-compose down
   ```
```

**Acceptance Criteria:**
- ‚úÖ docker-compose.yml created
- ‚úÖ Two services defined: forecast + web
- ‚úÖ Environment variables configured via .env
- ‚úÖ Volumes mounted for persistence
- ‚úÖ Health checks configured
- ‚úÖ Services start successfully
- ‚úÖ Web viewer accessible
- ‚úÖ README.md updated with Docker instructions

**Verification:**
```bash
# Compose file valid
docker-compose config
# Expected: No errors

# Services start
docker-compose up -d
docker-compose ps
# Expected: Both services "Up"

# Web accessible
curl http://localhost:8000/health
# Expected: Success

# Cleanup
docker-compose down
```

---

### Task 2.3: Create systemd Unit Files

**Agent:** deployment-engineer
**Priority:** MEDIUM
**Effort:** 1 hour
**Blocking:** Systemd deployment option

**Instructions:**

1. **Create deployment/systemd/ directory:**
   ```bash
   mkdir -p deployment/systemd
   ```

2. **Create deployment/systemd/surfcastai-forecast.service:**

```ini
[Unit]
Description=SurfCastAI Forecast Generation Service
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
User=surfcast
Group=surfcast
WorkingDirectory=/opt/surfCastAI
EnvironmentFile=/opt/surfCastAI/.env
ExecStart=/opt/surfCastAI/venv/bin/python src/main.py run --mode full

# Security hardening
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/opt/surfCastAI/data /opt/surfCastAI/output /opt/surfCastAI/logs

# Logging
StandardOutput=append:/opt/surfCastAI/logs/forecast-service.log
StandardError=append:/opt/surfCastAI/logs/forecast-service.log

# Resource limits
MemoryMax=2G
CPUQuota=200%

[Install]
WantedBy=multi-user.target
```

3. **Create deployment/systemd/surfcastai-forecast.timer:**

```ini
[Unit]
Description=SurfCastAI Forecast Generation Timer
Requires=surfcastai-forecast.service

[Timer]
# Run daily at 6 AM HST (16:00 UTC)
OnCalendar=daily
OnCalendar=16:00
Persistent=true
RandomizedDelaySec=300

[Install]
WantedBy=timers.target
```

4. **Create deployment/systemd/surfcastai-web.service:**

```ini
[Unit]
Description=SurfCastAI Web Viewer Service
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
User=surfcast
Group=surfcast
WorkingDirectory=/opt/surfCastAI
EnvironmentFile=/opt/surfCastAI/.env
Environment="SURFCAST_OUTPUT_DIR=/opt/surfCastAI/output"

ExecStart=/opt/surfCastAI/venv/bin/uvicorn src.web.app:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --log-level info

# Restart on failure
Restart=on-failure
RestartSec=5s

# Security hardening
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadOnlyPaths=/opt/surfCastAI/output

# Resource limits
MemoryMax=1G
CPUQuota=150%

[Install]
WantedBy=multi-user.target
```

5. **Create deployment/systemd/README.md:**

```markdown
# SurfCastAI systemd Services

## Installation

1. **Copy service files:**
   ```bash
   sudo cp deployment/systemd/*.service /etc/systemd/system/
   sudo cp deployment/systemd/*.timer /etc/systemd/system/
   ```

2. **Reload systemd:**
   ```bash
   sudo systemctl daemon-reload
   ```

3. **Enable services:**
   ```bash
   # Enable forecast timer (runs daily)
   sudo systemctl enable surfcastai-forecast.timer
   sudo systemctl start surfcastai-forecast.timer

   # Enable web viewer (always running)
   sudo systemctl enable surfcastai-web.service
   sudo systemctl start surfcastai-web.service
   ```

4. **Check status:**
   ```bash
   # Timer status
   sudo systemctl status surfcastai-forecast.timer
   sudo systemctl list-timers surfcastai-forecast.timer

   # Web service status
   sudo systemctl status surfcastai-web.service

   # View logs
   sudo journalctl -u surfcastai-forecast.service -f
   sudo journalctl -u surfcastai-web.service -f
   ```

## Management

```bash
# Manually trigger forecast
sudo systemctl start surfcastai-forecast.service

# Restart web viewer
sudo systemctl restart surfcastai-web.service

# Stop services
sudo systemctl stop surfcastai-forecast.timer
sudo systemctl stop surfcastai-web.service

# Disable services
sudo systemctl disable surfcastai-forecast.timer
sudo systemctl disable surfcastai-web.service
```
```

**Acceptance Criteria:**
- ‚úÖ 3 systemd unit files created (forecast.service, forecast.timer, web.service)
- ‚úÖ Security hardening applied (NoNewPrivileges, ProtectSystem, ReadWritePaths)
- ‚úÖ Resource limits configured (MemoryMax, CPUQuota)
- ‚úÖ Logging configured to files
- ‚úÖ Timer set for daily 6 AM HST (16:00 UTC)
- ‚úÖ README.md with installation/management instructions
- ‚úÖ Files committed to repository

**Verification:**
```bash
# Validate service files
systemd-analyze verify deployment/systemd/*.service
systemd-analyze verify deployment/systemd/*.timer
# Expected: No errors

# Check all required fields present
grep -E "Description|ExecStart|User" deployment/systemd/*.service
# Expected: All services have required fields
```

---

**PHASE 2 COMPLETION CHECKLIST:**
- [ ] Task 2.1: Dockerfile created and tested
- [ ] Task 2.2: docker-compose.yml created and tested
- [ ] Task 2.3: systemd unit files created
- [ ] Verification: `docker build -t surfcastai:latest .` succeeds
- [ ] Verification: `docker-compose up -d` starts both services
- [ ] Verification: systemd files pass `systemd-analyze verify`
- [ ] Verification: README.md updated with deployment instructions

**AFTER PHASE 2:** System is DEPLOYMENT-READY (containerization: ‚úÖ, systemd: ‚úÖ)

---

## Phase 3: TEST COVERAGE IMPROVEMENT (OPTIONAL - Long-term Quality) üìà

**Priority:** ONGOING (improve over time)
**Timeline:** 1-2 weeks
**Goal:** Raise test coverage from 66% to 90% (target), focusing on critical paths

**Note:** Current 96.6% pass rate is production-acceptable. The 66% coverage is lower than ideal but doesn't indicate bugs - it indicates untested edge cases. This phase systematically increases coverage for long-term maintainability, but the system is functional and reliable at current coverage levels.

### Coverage Priority Matrix

| Component | Current | Target | Priority | Effort |
|-----------|---------|--------|----------|--------|
| forecast_engine.py | 29% | 80% | CRITICAL | 2-3 days |
| data_fusion_system.py | 57% | 80% | HIGH | 1-2 days |
| main.py | 14% | 60% | HIGH | 1 day |
| bundle_manager.py | 58% | 80% | MEDIUM | 1 day |
| metadata_tracker.py | 14% | 70% | MEDIUM | 1 day |
| hawaii_context.py | 47% | 70% | LOW | 1 day |
| prompt_templates.py | 34% | 60% | LOW | 0.5 day |

**Total Effort:** 8-10 days (can be parallelized across multiple agents)

---

### Task 3.1: Write Tests for ForecastEngine (29% ‚Üí 80%)

**Agent:** test-guardian
**Priority:** CRITICAL
**Effort:** 2-3 days
**Blocking:** Core functionality confidence

**Problem:**
ForecastEngine (909 lines) only has 29% coverage, leaving 288 statements untested.

**Instructions:**

1. **Analyze current test coverage:**
   ```bash
   pytest tests/unit/forecast_engine/ --cov=src/forecast_engine/forecast_engine --cov-report=html
   open htmlcov/index.html  # Review uncovered lines
   ```

2. **Identify critical uncovered paths:**
   - Storm detection integration (lines 486-507)
   - Validation feedback integration (lines 802-831)
   - Image analysis (lines 458-640)
   - Specialist workflow (lines 274-292, commented out)
   - Error handling paths

3. **Create test file structure:**
   ```bash
   mkdir -p tests/unit/forecast_engine

   # Create test files
   touch tests/unit/forecast_engine/test_forecast_engine_core.py
   touch tests/unit/forecast_engine/test_forecast_engine_specialists.py
   touch tests/unit/forecast_engine/test_forecast_engine_integration.py
   ```

4. **Write core functionality tests:**

```python
# tests/unit/forecast_engine/test_forecast_engine_core.py

import pytest
from unittest.mock import Mock, AsyncMock, patch
from src.forecast_engine.forecast_engine import ForecastEngine
from src.core.config import Config

@pytest.fixture
def mock_config():
    config = Mock(spec=Config)
    config.get.side_effect = lambda section, key, default=None: {
        ('openai', 'model'): 'gpt-4o-mini',
        ('openai', 'api_key'): 'sk-test',
        ('forecast', 'use_specialist_team'): False,
        ('forecast', 'timeout'): 300,
    }.get((section, key), default)
    return config

@pytest.fixture
def forecast_engine(mock_config):
    return ForecastEngine(mock_config)

class TestForecastEngineInit:
    def test_initialization(self, forecast_engine, mock_config):
        """Test ForecastEngine initializes correctly"""
        assert forecast_engine.config == mock_config
        assert forecast_engine.client is not None

    def test_model_configuration(self, forecast_engine):
        """Test model is configured from config"""
        assert forecast_engine.model == 'gpt-4o-mini'

    def test_specialist_team_disabled_by_default(self, forecast_engine):
        """Test specialist team is disabled when config says False"""
        assert forecast_engine.use_specialist_team is False

class TestTokenBudgetManagement:
    def test_calculate_token_budget(self, forecast_engine):
        """Test token budget calculation"""
        # Mock processed data
        processed_data = {
            'buoy_events': [{'buoy_id': '51201'}] * 5,
            'weather_data': [{}] * 3,
        }

        budget = forecast_engine._calculate_token_budget(processed_data)
        assert isinstance(budget, int)
        assert budget > 0

    def test_token_budget_scales_with_data_size(self, forecast_engine):
        """Test token budget increases with more data"""
        small_data = {'buoy_events': [{}] * 2}
        large_data = {'buoy_events': [{}] * 10}

        small_budget = forecast_engine._calculate_token_budget(small_data)
        large_budget = forecast_engine._calculate_token_budget(large_data)

        assert large_budget > small_budget

class TestStormDetectionIntegration:
    @patch('src.forecast_engine.forecast_engine.StormDetector')
    def test_storm_detection_called(self, mock_detector, forecast_engine):
        """Test storm detection is called with pressure analysis"""
        mock_detector_instance = Mock()
        mock_detector_instance.detect_storms.return_value = []
        mock_detector.return_value = mock_detector_instance

        pressure_analysis = "Low pressure system..."
        result = forecast_engine._detect_storms(pressure_analysis)

        mock_detector_instance.detect_storms.assert_called_once()
        assert isinstance(result, list)

    @patch('src.forecast_engine.forecast_engine.StormDetector')
    def test_storm_detection_handles_errors(self, mock_detector, forecast_engine):
        """Test storm detection gracefully handles errors"""
        mock_detector_instance = Mock()
        mock_detector_instance.detect_storms.side_effect = Exception("Parse error")
        mock_detector.return_value = mock_detector_instance

        result = forecast_engine._detect_storms("Invalid input")

        # Should return empty list on error (graceful degradation)
        assert result == []

class TestValidationFeedbackIntegration:
    @patch('src.forecast_engine.forecast_engine.ValidationFeedback')
    def test_validation_feedback_queried(self, mock_feedback, forecast_engine):
        """Test validation feedback is queried"""
        mock_feedback_instance = Mock()
        mock_feedback_instance.generate_adaptive_context.return_value = {
            'north_shore': 'Historical MAE: 1.2ft',
            'south_shore': 'Historical MAE: 0.8ft',
        }
        mock_feedback.return_value = mock_feedback_instance

        context = forecast_engine._get_validation_feedback()

        mock_feedback_instance.generate_adaptive_context.assert_called_once()
        assert 'north_shore' in context
        assert 'south_shore' in context

    @patch('src.forecast_engine.forecast_engine.ValidationFeedback')
    def test_validation_feedback_handles_no_data(self, mock_feedback, forecast_engine):
        """Test validation feedback handles insufficient data gracefully"""
        mock_feedback_instance = Mock()
        mock_feedback_instance.generate_adaptive_context.return_value = {}
        mock_feedback.return_value = mock_feedback_instance

        context = forecast_engine._get_validation_feedback()

        # Should return empty dict, not crash
        assert context == {}

# Add 50+ more tests to reach 80% coverage
# Focus on:
# - Image analysis error handling
# - API timeout handling
# - Token budget edge cases
# - Specialist workflow (when re-enabled)
# - Confidence scoring
# - Output formatting
```

5. **Run tests and measure coverage:**
   ```bash
   pytest tests/unit/forecast_engine/test_forecast_engine_core.py -v --cov=src/forecast_engine/forecast_engine --cov-report=term-missing

   # Check coverage improvement
   # Target: 80% (currently 29%)
   ```

**Acceptance Criteria:**
- ‚úÖ Test file created: test_forecast_engine_core.py
- ‚úÖ 50+ tests written covering critical paths
- ‚úÖ Coverage increased from 29% to ‚â•80%
- ‚úÖ All tests passing
- ‚úÖ No regressions in existing tests

**Verification:**
```bash
# Run all forecast engine tests
pytest tests/unit/forecast_engine/ -v

# Check coverage
pytest tests/unit/forecast_engine/ --cov=src/forecast_engine/forecast_engine --cov-report=term
# Expected: Coverage ‚â•80%
```

---

### Task 3.2: Write Tests for DataFusionSystem (57% ‚Üí 80%)

**Agent:** test-guardian
**Priority:** HIGH
**Effort:** 1-2 days
**Dependencies:** None

**Instructions:**

Similar structure to Task 3.1, focusing on:
- Multi-component swell separation (lines 440-485)
- Cross-buoy validation (lines 604-638)
- Quality filtering (lines 575-670)
- Event merging (lines 769-827)
- Spectral analyzer integration

**Detailed instructions omitted for brevity - follow same pattern as Task 3.1**

---

### Task 3.3: Write Tests for main.py (14% ‚Üí 60%)

**Agent:** test-guardian
**Priority:** HIGH
**Effort:** 1 day
**Dependencies:** None

**Instructions:**

Focus on CLI argument parsing and command execution:
- Argument parser validation
- Mode selection (collect, process, forecast, full)
- Bundle ID handling
- Config validation flow
- Error handling and exit codes

**Detailed instructions omitted for brevity - follow same pattern as Task 3.1**

---

**PHASE 3 COMPLETION CHECKLIST:**
- [ ] Task 3.1: ForecastEngine tests (29% ‚Üí 80%)
- [ ] Task 3.2: DataFusionSystem tests (57% ‚Üí 80%)
- [ ] Task 3.3: main.py tests (14% ‚Üí 60%)
- [ ] Task 3.4: BundleManager tests (58% ‚Üí 80%)
- [ ] Task 3.5: MetadataTracker tests (14% ‚Üí 70%)
- [ ] Verification: Overall coverage increased from 66% to ‚â•80%
- [ ] Verification: All new tests passing
- [ ] Verification: No regressions in existing tests

**AFTER PHASE 3:** System is WELL-TESTED (coverage: 80%+, confidence: HIGH)

---

## Phase 4: OPTIONAL IMPROVEMENTS (DEFERRED) üí°

**These are nice-to-have enhancements that can be deferred to future iterations.**

### Task 4.1: Migrate Core Models to Pydantic

**Agent:** python-pro
**Priority:** LOW
**Effort:** 1-2 weeks
**Value:** Runtime validation, IDE autocomplete, type safety

**Scope:**
- Migrate BuoyData + BuoyObservation (buoy_data.py)
- Migrate WeatherData + WeatherPeriod (weather_data.py)
- Migrate SwellEvent + SwellComponent (swell_event.py)
- Migrate ModelData + ModelForecast (wave_model.py)

**Why deferred:** Current dataclasses work fine, migration is time-intensive

---

### Task 4.2: Refactor Large Methods

**Agent:** code-reviewer
**Priority:** LOW
**Effort:** 1 week
**Value:** Maintainability, testability

**Scope:**
- forecast_engine.py::_generate_main_forecast() (241 lines ‚Üí 3 methods)
- data_fusion_system.py::_extract_buoy_events() (206 lines ‚Üí 4 methods)
- buoy_processor.py::_analyze_buoy_data() (179 lines ‚Üí 3 methods)

**Why deferred:** Code works, refactoring is not urgent

---

### Task 4.3: Expand CI/CD Pipeline

**Agent:** deployment-engineer
**Priority:** LOW
**Effort:** 1 week
**Value:** Automation, quality gates

**Scope:**
- Add GitHub Actions test workflow
- Add linting workflow (black, mypy, flake8)
- Add Docker build workflow
- Add deployment workflow

**Why deferred:** Manual deployment works, automation can wait

---

### Task 4.4: Add Prometheus Metrics

**Agent:** deployment-engineer
**Priority:** LOW
**Effort:** 2-3 days
**Value:** Observability, performance monitoring

**Scope:**
- Add /metrics endpoint to web viewer
- Track forecast generation rate, duration, errors
- Track validation accuracy (MAE, RMSE by shore)
- Create Grafana dashboard

**Why deferred:** Basic monitoring (Healthchecks.io) is sufficient initially

---

## Task Execution Protocol

### For User:

1. **Before starting any phase:**
   - Review task list
   - Confirm priorities and effort estimates
   - Assign agents or approve agent assignments

2. **During execution:**
   - Monitor agent progress
   - Review agent outputs before accepting changes
   - Enforce strict adherence to TODO instructions
   - **STOP immediately on any deviation** from TODO

3. **After each task:**
   - Verify acceptance criteria met
   - Run verification commands
   - Update task checklist
   - Commit changes with descriptive messages

4. **After each phase:**
   - Review phase completion checklist
   - Run full test suite
   - Update project documentation
   - Plan next phase

### For Agents:

1. **Read task instructions completely** before starting
2. **Follow instructions exactly** - no improvisation
3. **Report findings** before making changes
4. **Wait for approval** before modifying files
5. **Verify work** using provided commands
6. **Report completion** with acceptance criteria checklist

---

## Quick Reference

### Current Status (October 14, 2025)

| Metric | Status |
|--------|--------|
| Security | ‚ö†Ô∏è 3 CRITICAL ISSUES (Phase 0) |
| Tests | ‚ö†Ô∏è 982/1,017 passing (96.6%), 35 failing |
| Coverage | ‚ö†Ô∏è 66% (target: 90%) |
| Documentation | ‚ö†Ô∏è OUTDATED (needs Phase 1.3-1.5) |
| Deployment | ‚úÖ 75% ready (manual works, needs containers) |
| Core Functionality | ‚úÖ PRODUCTION-READY |

### Priority Order

1. **CRITICAL (Do First):** Phase 0 - Security fixes (1 hour)
2. **HIGH (Do Soon):** Phase 1 - Stabilization (1-2 days)
3. **MEDIUM (Do This Week):** Phase 2 - Deployment (2-3 days)
4. **ONGOING (Do This Month):** Phase 3 - Test coverage (1-2 weeks)
5. **OPTIONAL (Defer):** Phase 4 - Nice-to-have improvements

### Time to Production

- **Minimal (Manual Deploy):** 1 hour (Phase 0 only)
- **Stable (Recommended):** 1.5 days (Phase 0 + Phase 1)
- **Container-Ready:** 4 days (Phase 0 + Phase 1 + Phase 2)
- **Well-Tested:** 2-3 weeks (Phase 0 + Phase 1 + Phase 2 + Phase 3)

---

## Notes

- This TODO is based on comprehensive 5-phase project review (Oct 14, 2025)
- All findings documented in generated audit reports (see root directory)
- Task estimates are for focused work; can be parallelized across agents
- Acceptance criteria must be met before marking tasks complete
- Any deviations from instructions require explicit approval

**Last Updated:** October 14, 2025
**Next Review:** After Phase 1 completion
**Status:** ‚úÖ COMPREHENSIVE ROADMAP READY
